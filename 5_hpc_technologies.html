<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Introduction to High Performance Computing: Landscape of HPC Technologies</title><meta name="viewport" content="width=device-width, initial-scale=1"><script src="assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="assets/styles.css"><script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="favicons/incubator/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="favicons/incubator/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="favicons/incubator/favicon-16x16.png"><link rel="manifest" href="favicons/incubator/site.webmanifest"><link rel="mask-icon" href="favicons/incubator/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="black"></head><body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Lesson Description" src="assets/images/incubator-logo.svg"><span class="badge text-bg-danger">
          <abbr title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.">
            <a href="https://docs.carpentries.org/resources/curriculum/lesson-life-cycle.html" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
              Pre-Alpha
            </a>
            <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text"><li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul></li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/5_hpc_technologies.html';">Instructor View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Lesson Description" src="assets/images/incubator-logo-sm.svg"></div>
    <div class="lesson-title-md">
      Introduction to High Performance Computing
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            Introduction to High Performance Computing
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"><li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Introduction to High Performance Computing
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 100%" class="percentage">
    100%
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: 100%" aria-valuenow="100" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text"><li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul></li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/5_hpc_technologies.html">Instructor View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->

            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="1_intro_to_hpc_systems.html">1. Introduction to HPC Systems</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="2_using_hpc_systems.html">2. Accessing and Using HPC Resources</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="3_job_scheduling.html">3. Introduction to Job Scheduling</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="4_programmatic_parallelism.html">4. Introduction to Programmatic Parallelism</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        5. Landscape of HPC Technologies
        </span>
      </button>
    </div><!--/div.accordion-header-->

    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#common-programming-languages">Common programming languages</a></li>
<li><a href="#a-simple-example">A simple example</a></li>
<li><a href="#openmp">OpenMP</a></li>
<li><a href="#message-passing-interface-mpi">Message Passing Interface (MPI)</a></li>
<li><a href="#using-gpus-instead-of-cpus">Using GPUs instead of CPUs</a></li>
<li><a href="#openacc">OpenACC</a></li>
<li><a href="#cuda">CUDA</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="6_parallel_scaling.html">6. Measuring and improving parallel performance</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>
                      <li><a href="reference.html">Reference</a></li>
                    </ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources"><a href="aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">

            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="4_programmatic_parallelism.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="6_parallel_scaling.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="4_programmatic_parallelism.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Introduction to
        </a>
        <a class="chapter-link float-end" href="6_parallel_scaling.html" rel="next">
          Next: Measuring and...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>Landscape of HPC Technologies</h1>
        <p>Last updated on 2025-11-06 |

        <a href="https://github.com/carpentries/workbench-template-md/edit/main/episodes/5_hpc_technologies.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>



        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>Which programming language should I use?</li>
<li>What are the main ways we can parallelise a program on modern HPC
systems?</li>
<li>How do OpenMP and MPI differ in how they achieve parallelism?</li>
<li>What role do GPUs play in HPC, and how do approaches like CUDA and
OpenACC use them?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul><li>Differentiate at a high level between the features of OpenMP, MPI,
CUDA and OpenACC</li>
<li>Briefly summarise the main OpenMP compiler directives and what they
do</li>
<li>Describe how to compile and run an OpenMP program</li>
<li>Briefly summarise the main MPI message-passing features and how they
are used</li>
<li>Describe how to compile and run an MPI program</li>
<li>Briefly summarise how an OpenACC program is written, compiled and
run</li>
<li>Briefly summarise how a CUDA program is written, compiled and
run</li>
</ul></div>
</div>
</div>
</div>
</div>
<p>To get the most performance possible, high-performance computing
relies on parallelism. Modern systems combines tens of thousands of
processors, each working on their own part of the problem. To be able to
wield this power, we need to understand how to write parallel code. This
is far more complicated than the scope of this episode. Therefore we
will, instead, look at the landscape of the core HPC technologies used
today in research. In particular, we will look at the following parallel
frameworks: OpenMP, MPI, OpenACC and CUDA.</p>
<section><h2 class="section-heading" id="common-programming-languages">Common programming languages<a class="anchor" aria-label="anchor" href="#common-programming-languages"></a></h2>
<hr class="half-width"><p>Before we start, let’s look at what programming languages are used in
HPC. In principle, any programming language can be used to write code
which runs on an HPC cluster. In practise, however, there are some
languages which are better suited to writing highly performant code.
Interpreted languages, such as <a href="https://www.python.org/" class="external-link">Python</a>, are easy to develop in, but
are much slower to execute than compiled languages, making them less
suitable for computation-heavy applications. For this reason, compiled
languages such as <a href="https://en.wikipedia.org/wiki/C_(programming_language)" class="external-link">C</a>, <a href="https://en.wikipedia.org/wiki/C%2B%2B" class="external-link">C++</a> and <a href="https://fortran-lang.org/" class="external-link">Fortran</a> are some of the most common
choices of programming language when writing high performance
applications as they produce fast, optimised executables.</p>
<p>Python does, however, remain widely used in HPC, typically by
performance-critical sections of code being written in compiled
languages and accessed through a Python interface. Frameworks such as <a href="https://pypi.org/project/torch/" class="external-link">PyTorch</a>, <a href="https://pypi.org/project/numpy/" class="external-link">NumPy</a>, <a href="https://pypi.org/project/scipy/" class="external-link">SciPy</a> and <a href="https://pypi.org/project/numba/" class="external-link">Numba</a> all rely on underlying
C, Fortran and/or CUDA to accelerate computation, while libraries such
as <a href="https://pypi.org/project/mpi4py/" class="external-link">mpi4py</a> or <a href="https://docs.python.org/3/library/multiprocessing.html" class="external-link">multiprocessing</a>
make it possible for distributed parallelism directly in Python. Hybrid
approaches like this have become more common because they combine
Python’s ease of use with the speed of compiled code.</p>
<p>Other specialised languages and frameworks are also used in HPC,
depending on the application area. <a href="https://developer.nvidia.com/cuda-toolkit" class="external-link">CUDA</a> and <a href="https://www.amd.com/en/products/software/rocm.html" class="external-link">ROCm</a> are
employed for GPU programming, while newer languages such as <a href="https://julialang.org/" class="external-link">Julia</a> aim to combine ease of
development with high performance. Domain-specific tools like <a href="https://www.mathworks.com/products/matlab.html" class="external-link">MATLAB</a>, and <a href="https://www.r-project.org/" class="external-link">R</a> also appear in HPC environments,
though they often rely on compiled extensions or external libraries for
parallel execution.</p>
<p>So which language should you use? There is no simple answer. The best
choice depends heavily on your specific application, the target hardware
(like CPUs or GPUs), and the trade-offs you are willing to make between
raw computational performance and ease of development. Ultimately, the
right language is the one that allows you to solve your problem
efficiently, leveraging the available libraries and expertise within
your team.</p>
<div id="which-compiler-should-you-use" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="which-compiler-should-you-use" class="callout-inner">
<h3 class="callout-title">Which compiler should you use?</h3>
<div class="callout-content">
<p>Iridis and most HPC systems offer a choice of compilers to use. But
which one should you use? In most cases, you would want to use the
compiler specific to the type of CPU you are using on the system,
because these implement architecture specific optimisations e.g. use
AMD’s <code>aocc</code> on Iridis 6. If you are using NVIDIA GPUs, you
have little choice in using anything other than NVIDIA’s
<code>nvc</code>, <code>nvcc</code> or <code>nvfortran</code>
compilers.</p>
<p>However, the code you are using may only have been tested on a
specific compiler such as <a href="https://gcc.gnu.org/" class="external-link">GCC</a>. In
those cases, it’s often best to stick with what is known to work.
However, there is nothing stopping you using Intel’s compilers on an AMD
based system, if the code is only tested or depends on features in the
Intel compilers.</p>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="a-simple-example">A simple example<a class="anchor" aria-label="anchor" href="#a-simple-example"></a></h2>
<hr class="half-width"><p>We’ll now take a high level look at a selection of some of the most
used libraries and frameworks used to parallelise code in research. In
particular, we will see how to use them, the code changes required and
how to run them on Iridis. The intention is not to make you proficient
with these frameworks–or even dangerous–but to give a high level
appreciation on what is being used.</p>
<p>To illustrate this, we will use a simple program, written in C, which
adds together two vectors to explore the code changes required. More
specifically, we will be modifying the following function
<code>vector_add</code> to run in parallel. We have chosen to use C as
it is commonly used language in HPC.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co">// *a, *b, and *c are arrays and n is the length of them.</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="co">// The result of the addition is returned back in *c</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="dt">void</span> vector_add<span class="op">(</span><span class="dt">int</span> <span class="op">*</span>a<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span>b<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span>c<span class="op">,</span> <span class="dt">int</span> n<span class="op">)</span> <span class="op">{</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>    <span class="co">// This is the loop which we'll parallelise</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> n<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>        c<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> a<span class="op">[</span>i<span class="op">]</span> <span class="op">+</span> b<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<p>You can find the entire program in <a href="files/vector/vector_serial.c">vector_serial.c</a>. To run this
code on Iridis X, we’ll use the following <a href="files/vector/submit_vector_serial.sh">submission script</a>.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="co">#SBATCH --partition=amd</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="co">#SBATCH --nodes=1</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="co">#SBATCH --ntasks=1</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="co">#SBATCH --time=00:01:00</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a><span class="co"># Load the gcc module, which is the compiler we'll use</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a><span class="ex">module</span> load gcc</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a><span class="co"># Compile the program using gcc</span></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a><span class="fu">gcc</span> vector_serial.c <span class="at">-o</span> vector_serial.exe</span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a><span class="co"># Run the compiled executable</span></span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a><span class="ex">./vector_serial.exe</span></span></code></pre>
</div>
<p>After the program has run, we should see the following output.</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Verification (first 5 elements):
c[0] =   0 (expected:   0)
c[1] =   3 (expected:   3)
c[2] =   6 (expected:   6)
c[3] =   9 (expected:   9)
c[4] =  12 (expected:  12)</code></pre>
</div>
</section><section><h2 class="section-heading" id="openmp">OpenMP<a class="anchor" aria-label="anchor" href="#openmp"></a></h2>
<hr class="half-width"><p>The first framework we’ll look at is OpenMP. As mentioned in the
previous episode, OpenMP is an industry-standard framework designed for
parallel programming in a shared-memory environment. OpenMP spawns
threads with each one, ideally, running on its own CPU core. OpenMP
works by using <em>compiler directives</em> to tell the compiler which
code needs to be parallelised, letting OpenMP and the compiler take care
of all the <em>low-level</em> parallelisation details. In general, you
just need to say which parts of your code you want to run in
parallel.</p>
<div id="compiler-directives" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="compiler-directives" class="callout-inner">
<h3 class="callout-title">Compiler directives</h3>
<div class="callout-content">
<p>If you’re unfamiliar with compiler directives, you can think of them
as being a <em>special command</em> for the compiler, not for the
program itself. In C and C++, these almost <em>always</em> start with
<code>#pragma</code>. Think of it as a special note to the compiler
which says, “when you compile this specific piece of code, do something
extra.” Since these are compiler options, they do not modify the run
time behaviour of the program, only how the program is compiled.</p>
</div>
</div>
</div>
<p>Parallelisation with OpenMP is handled with these compiler
directives. However, OpenMP does also offer a library of runtime
functions which gives finer grained control, such as if you need to
ensure thread synchronisation or need to go off the beaten track. To
parallelise our <code>vector_add</code> function, we only need to add a
single line of code, using a compiler directive, just before the
loop.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="dt">void</span> vector_add<span class="op">(</span><span class="dt">int</span> <span class="op">*</span>a<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span>b<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span>c<span class="op">,</span> <span class="dt">int</span> n<span class="op">)</span> <span class="op">{</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="co">// This directive tells OpenMP to spawn threads and to</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a><span class="co">// divide the loop iterations between them. Each thread will</span></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a><span class="co">// handle a fraction of the loop iterations/vector addition</span></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a><span class="pp">#pragma omp parallel for</span></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> n<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>        c<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> a<span class="op">[</span>i<span class="op">]</span> <span class="op">+</span> b<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<p>The full program is in <a href="files/vector/vector_openmp.c">vector_openmp.c</a>. Let’s break
this down more. The directive we used,
<code>#pragma omp parallel for</code>, tells the compiler that the next
for loop should be parallelised. The compiler then automatically
parallelises it for us, creating a team of threads and dividing the
loop’s work among them. In this case, each thread will perform a portion
of the vector addition. All OpenMP directives begin with
<code>#pragma omp</code>, followed by a specific command.</p>
<p>There are lots of other directives available, with
<code>#pragma omp parallel for</code> being the most commonly used.
Another useful directive is <code>#pragma omp atomic</code> which
prevents multiple threads from modifying a variable at once. This is one
way to prevent the race conditions mentioned in the previous episode.
OpenMP also provides a library of runtime functions which offers even
more control. For example, we can use the function
<code>omp_set_num_threads()</code> to control the number of threads that
OpenMP will spawn. In C, we need to include the appropriate header file,
<code>&lt;omp.h&gt;</code>, to access the library functions.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;omp.h&gt;</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="dt">void</span> vector_add<span class="op">(</span><span class="dt">int</span> <span class="op">*</span>a<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span>b<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span>c<span class="op">,</span> <span class="dt">int</span> n<span class="op">)</span> <span class="op">{</span></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>    <span class="co">// Manually set the number of threads to 8</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>    omp_set_num_threads<span class="op">(</span><span class="dv">8</span><span class="op">);</span></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a><span class="pp">#pragma omp parallel for</span></span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> n<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>        c<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> a<span class="op">[</span>i<span class="op">]</span> <span class="op">+</span> b<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<div id="directives-and-functions-for-synchronisation" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="directives-and-functions-for-synchronisation" class="callout-inner">
<h3 class="callout-title">Directives and functions for
synchronisation</h3>
<div class="callout-content">
<p>Effective control of thread synchronisation is essential when
parallelising code with OpenMP, as improper handling of shared data can
lead to race conditions and unpredictable results. To support this,
OpenMP provides a range of directives and library functions that
coordinate access to shared data and manage thread behaviour. The tables
below summarise several commonly used examples.</p>
<table class="table"><colgroup><col width="14%"><col width="85%"></colgroup><thead><tr class="header"><th>Compiler Directive</th>
<th>Description</th>
</tr></thead><tbody><tr class="odd"><td><code>#pragma omp atomic</code></td>
<td>Ensures a specific operation, such as modifying a variable, is
executed atomically, e.g. by one thread at a time, to prevent race
conditions.</td>
</tr><tr class="even"><td><code>#pragma omp critical</code></td>
<td>Defines a region of code that only one thread can execute at a
time.</td>
</tr></tbody></table><p>This list is not exhaustive. A complete reference for all OpenMP
directives and functions is available in the <a href="https://www.openmp.org/wp-content/uploads/OpenMP-RefGuide-6.0-OMP60SC24-web.pdf" class="external-link">OpenMP
6.0 Reference Guide</a>.</p>
</div>
</div>
</div>
<p>To compile an OpenMP program, we need to use the
<code>-fopenmp</code> flag,
e.g. <code>gcc -fopenmp vector_openmp.c</code>. If we don’t use the
<code>-fopenmp</code> flag, the compiler directives are ignored and any
library functions from <code>&lt;omp.h&gt;</code> will not be found
resulting in a compilation error.</p>
<div id="do-i-need-to-keep-the-serial-version" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="do-i-need-to-keep-the-serial-version" class="callout-inner">
<h3 class="callout-title">Do I need to keep the serial version?</h3>
<div class="callout-content">
<p>Setting the number of threads or processes to one will produce the
same behaviour as the serial program, assuming no programming errors.
However, to be extra safe, you can use <a href="https://en.wikipedia.org/wiki/Conditional_compilation" class="external-link">conditional
compilation</a> to maintain both parallel and serial versions within the
same codebase/file. Compiled languages support conditional compilation,
which allows certain sections of code to be compiled only if a specific
condition is met. When using OpenMP, the compiler variable
<code>_OPENMP</code> is defined when the <code>-fopenmp</code> flag is
passed, so you can use this variable to prevent OpenMP directives and
functions from being compiled when <code>-fopenmp</code> is not
used.</p>
</div>
</div>
</div>
<p>To launch an OpenMP program, run it like any other program. The
number of threads for OpenMP to use can be controlled using the
environment variable <code>OMP_NUM_THREADS</code>, or the
<code>omp_set_num_threads()</code> function discussed earlier. If the
function is unused and the environment variable left unset, OpenMP will
spawn one thread per CPU core. Normally, on a HPC cluster this is
probably what is wanted. However, when running on your own computer
during development and testing, you will probably want to set
<code>OMP_NUM_THREADS</code> to be less than then number of CPU cores to
avoid overloading your computer.</p>
<p>The following is an example of how you would compile and launch an
OpenMP program on Iridis X, in a single submission script.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a><span class="co">#SBATCH --partition=amd</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a><span class="co">#SBATCH --nodes=1</span></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a><span class="co">#SBATCH --ntasks=1</span></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a><span class="co">#SBATCH --cpus-per-task=8</span></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a><span class="co">#SBATCH --time=00:01:00</span></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a><span class="co"># Use OMP_NUM_THREADS to say we can to use --cpus-per-task number of</span></span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a><span class="co"># threads. The --cpu-per-task SBATCH directive is populated into the</span></span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a><span class="co"># SLURM_CPUS_PER_TASK environment variable</span></span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a><span class="bu">export</span> <span class="va">OMP_NUM_THREADS</span><span class="op">=</span><span class="va">$SLURM_CPUS_PER_TASK</span></span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a><span class="co"># Compile the program using gcc</span></span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a><span class="ex">module</span> load gcc</span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a><span class="fu">gcc</span> <span class="at">-fopenmp</span> vector_openmp.c <span class="at">-o</span> vector_openmp.exe</span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a><span class="co"># We run the compiled executable just like the serial version</span></span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a><span class="ex">./vector_openmp.exe</span></span></code></pre>
</div>
<p>The main advantage of OpenMP is that it requires only minimal code
modification to parallelise existing programs, particularly when the
main computational workload lies within loops. Beyond simple loop-level
parallelism, parallelising more complex program structures becomes more
challenging, though this is true of most parallel frameworks. OpenMP
still provides a wide range of straightforward directives, making it
easy to adapt serial code without needing to design the program for
parallel execution from the start. Its main limitation is that it uses a
shared-memory model, which requires careful management of thread
synchronisation and restricts scalability to a single compute node.</p>
</section><section><h2 class="section-heading" id="message-passing-interface-mpi">Message Passing Interface (MPI)<a class="anchor" aria-label="anchor" href="#message-passing-interface-mpi"></a></h2>
<hr class="half-width"><p>The Message Passing Interface (MPI) was designed to enable scientific
applications to run on supercomputers and, as such, has become the
dominant distributed-memory parallelism framework in research. An MPI
application creates many processes each working on their own individual
task. However, unlike shared-memory approaches, each MPI process–or
rank–has its own private memory space. Therefore to exchange data
between these ranks, explicit communication has to happen. This is where
MPI comes in. It provides a set of library functions to configure a
parallel environment and exchange data within it. The <em>key
concept</em> in MPI is message passing, which involves the explicit
exchange of data between processes. Processes can send messages to
specific destinations, broadcast messages to all processes, or perform
collective operations where all processes participate.</p>
<p>To achieve parallelism, each process runs a copy of the same program
as the other processes, but works on its own subset of data, or does its
own tasks. The processes communicate to exchange data and/or coordinate
their next tasks. The power of MPI is that the processes can run on
different nodes, allowing MPI programs to scale well beyond a single
machine. Thankfully the complexity of having to spawn processes on
different nodes and communicate over the network is hidden away and
controlled by MPI’s library of functions and the HPC cluster’s
scheduler/resource manager, e.g. Slurm on Iridis.</p>
<p>But there are some trade-offs with MPI. In terms of performance,
communicating data takes time. If you have a method which requires
frequent communication or data dependencies, then you will spend more
time exchanging data than doing computation. It may instead be worth
using a shared-memory framework like OpenMP. However the biggest
trade-off, by far, is that you need to design your application in mind
for MPI parallelisation. Unlike OpenMP, it is not simple to retrofit a
serial program with MPI. For example, we need to make sure we program
initialising the MPI library ourselves, track process IDs, explicitly
send and receive data, coordinate the processes so they do their own
work, and so on. The table below gives an idea of some of the MPI
functions we need to use and their purpose.</p>
<table class="table"><colgroup><col width="16%"><col width="83%"></colgroup><thead><tr class="header"><th>Function</th>
<th>Description</th>
</tr></thead><tbody><tr class="odd"><td><code>MPI_Init</code></td>
<td>Initialises the MPI environment. Must be called before any other MPI
calls.</td>
</tr><tr class="even"><td><code>MPI_Comm_size</code></td>
<td>Returns the total number of processes in the communicator.</td>
</tr><tr class="odd"><td><code>MPI_Comm_rank</code></td>
<td>Returns the rank (ID) of the calling process.</td>
</tr><tr class="even"><td>
<code>MPI_Send</code> / <code>MPI_Recv</code>
</td>
<td>Used for direct point-to-point communication between processes.</td>
</tr><tr class="odd"><td><code>MPI_Barrier</code></td>
<td>Used for synchronisation. A processes cannot continue past the
barrier until all processes have reached it.</td>
</tr><tr class="even"><td><code>MPI_Finalize</code></td>
<td>Shuts down the MPI environment. Must be called before exiting the
program, other wise background MPI tasks may not finish.</td>
</tr></tbody></table><p>This is only a tiny glimpse into the available MPI functions, and
what you can do with MPI. In total, there are around 500 functions.</p>
<p>Let’s now take a look at the code changes we need to parallelise the
<code>add_vector</code> function using MPI. A detailed description of
the MPI library functions, and how to design an MPI program, is out of
scope for this episode, and lesson, so we won’t go much into the
details. From a high level, what we need to do is: 1) initialise the MPI
environment, 2) split the work between processes, 3) have each process
do their work, and 4) communicate the results back to one or more
processes for the cycle to repeat. The example code below demonstrates
this. Keep in your head that each process is executes the
<strong>exact</strong> same function, but will do something different
based on its ID.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="co">//Include the MPI header file to access the MPI functions</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;mpi.h&gt;</span></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a><span class="dt">void</span> vector_add<span class="op">(</span><span class="dt">int</span> <span class="op">*</span>a<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span>b<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span>c<span class="op">,</span> <span class="dt">int</span> n<span class="op">)</span> <span class="op">{</span></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>    <span class="co">// Initialise the MPI environment -- nothing will work if we</span></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>    <span class="co">// don't do this</span></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>    MPI_Init<span class="op">(</span>NULL<span class="op">,</span> NULL<span class="op">);</span></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>    <span class="co">// Get the rank (process ID) and total number of processes launched</span></span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>    <span class="dt">int</span> rank<span class="op">,</span> size<span class="op">;</span></span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>    <span class="dt">int</span> root_rank <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>    MPI_Comm_rank<span class="op">(</span>MPI_COMM_WORLD<span class="op">,</span> <span class="op">&amp;</span>rank<span class="op">);</span></span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>    MPI_Comm_size<span class="op">(</span>MPI_COMM_WORLD<span class="op">,</span> <span class="op">&amp;</span>size<span class="op">);</span></span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a>    <span class="co">// Determine next how many elements of the vector this process</span></span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>    <span class="co">// will handle. We will create a "local" array which will store</span></span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a>    <span class="co">// the results of the vector addition for this process</span></span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a>    <span class="dt">int</span> n_local <span class="op">=</span> n <span class="op">/</span> size<span class="op">;</span></span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a>    <span class="dt">int</span> c_local<span class="op">[</span>n_local<span class="op">];</span></span>
<span id="cb7-20"><a href="#cb7-20" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" tabindex="-1"></a>    <span class="co">// Determine the subset of the vectors a and b this process</span></span>
<span id="cb7-22"><a href="#cb7-22" tabindex="-1"></a>    <span class="co">// will be adding together. To do this, we use the ID of</span></span>
<span id="cb7-23"><a href="#cb7-23" tabindex="-1"></a>    <span class="co">// the process and the number of elements each process</span></span>
<span id="cb7-24"><a href="#cb7-24" tabindex="-1"></a>    <span class="co">// will be working on</span></span>
<span id="cb7-25"><a href="#cb7-25" tabindex="-1"></a>    <span class="dt">int</span> start <span class="op">=</span> rank <span class="op">*</span> n_local<span class="op">;</span></span>
<span id="cb7-26"><a href="#cb7-26" tabindex="-1"></a>    <span class="dt">int</span> stop <span class="op">=</span> <span class="op">(</span>rank <span class="op">+</span> <span class="dv">1</span><span class="op">)</span> <span class="op">*</span> n_local<span class="op">;</span></span>
<span id="cb7-27"><a href="#cb7-27" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" tabindex="-1"></a>    <span class="co">// Perform the local computation on each process</span></span>
<span id="cb7-29"><a href="#cb7-29" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> start<span class="op">;</span> i <span class="op">&lt;</span> stop<span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb7-30"><a href="#cb7-30" tabindex="-1"></a>        c_local<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> a<span class="op">[</span>i<span class="op">]</span> <span class="op">+</span> b<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb7-31"><a href="#cb7-31" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb7-32"><a href="#cb7-32" tabindex="-1"></a></span>
<span id="cb7-33"><a href="#cb7-33" tabindex="-1"></a>    <span class="co">// We can use this MPI_Gather function to send the "partial" results</span></span>
<span id="cb7-34"><a href="#cb7-34" tabindex="-1"></a>    <span class="co">// from each process back to the "root" process. Only the root process</span></span>
<span id="cb7-35"><a href="#cb7-35" tabindex="-1"></a>    <span class="co">// will have the complete vector</span></span>
<span id="cb7-36"><a href="#cb7-36" tabindex="-1"></a>    MPI_Gather<span class="op">(</span>c_local<span class="op">,</span> n_local<span class="op">,</span> MPI_INT<span class="op">,</span> c<span class="op">,</span> n_local<span class="op">,</span> MPI_INT<span class="op">,</span></span>
<span id="cb7-37"><a href="#cb7-37" tabindex="-1"></a>               root_rank<span class="op">,</span> MPI_COMM_WORLD<span class="op">);</span></span>
<span id="cb7-38"><a href="#cb7-38" tabindex="-1"></a></span>
<span id="cb7-39"><a href="#cb7-39" tabindex="-1"></a>    <span class="co">// Finalise the MPI environment</span></span>
<span id="cb7-40"><a href="#cb7-40" tabindex="-1"></a>    MPI_Finalize<span class="op">();</span></span>
<span id="cb7-41"><a href="#cb7-41" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<p>The complete program is in <a href="files/vector/vector_mpi.c">vector_mpi.c</a>. As you can see, the
code is much more involved than the OpenMP example. Every process runs
this same function, but each one is responsible for a different part of
the vector addition. Each process follows a clear set of steps:</p>
<ul><li>First, it finds out its unique ID (<code>rank</code>) and the total
number of processes (<code>size</code>).</li>
<li>Using this, it calculates which “slice” of the vectors
<code>a</code> and <code>b</code> it is responsible for.</li>
<li>It then performs the addition for its slice and stores the partial
answer in a local <code>c_local</code> array.</li>
<li>Finally, it calls the <code>MPI_Gather</code> function, which
aggregates all the <code>c_local</code> arrays.</li>
</ul><p><code>MPI_Gather</code> is a collective communication shortcut.
Behind the curtain it doing the <code>MPI_Send</code> and
<code>MPI_Recv</code> for us, sending data from each process back to our
root process; which is the only process with the complete result.</p>
<div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Why does only the “root” process have the final, complete,
result?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>It is because we used <code>MPI_Gather</code> to “gather” the results
from each process back to the root process. There is no communication to
the other process, so only the root process has the data from the other
processes. If we wanted every process to have a copy of the final result
of <code>c</code>, we could instead use <code>MPI_Allgather</code>.</p>
</div>
</div>
</div>
</div>
<p>To compile an MPI application, we need to use an <em>MPI-aware</em>
compiler which is essentially a clever script which knows where MPI is
installed on our system to make compilation easier. On Iridis, this is
<code>mpicc</code>. We use <code>mpicc</code> as we would any other
compiler: <code>mpicc vector_mpi.c -o vector_mpi.exe</code>.</p>
<div id="whats-an-mpi-aware-compiler" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="whats-an-mpi-aware-compiler" class="callout-inner">
<h3 class="callout-title">What’s an MPI-aware compiler?</h3>
<div class="callout-content">
<p>An MPI-aware compiler is essentially a script which wraps around your
regular compiler to provide the required header files and libraries to
the compiler to build an MPI application. In theory, we could find those
headers and libraries ourselves and pass them the compiler instead.
However, in practise, this is not worth our time when <code>mpicc</code>
already does it for us.</p>
</div>
</div>
</div>
<p>To launch an MPI application we need to use <code>mpirun</code>,
e.g. <code>mpirun vector_mpi.exe</code>. This is another script provided
by MPI which handles launching the parallel processes and setting up the
parallel environment. If we run the program without using
<code>mpirun</code>, then only a single processes will start and will
probably become “deadlocked” as the program waits indefinitely for
messages from other processes that were never sent. On Iridis, and other
Slurm clusters, we can alternatively use the Slurm launcher
<code>srun</code> to do the same job. To summarise, the purposes of
<code>srun</code> and <code>mpirun</code> is to,</p>
<ol style="list-style-type: decimal"><li>Read the environment to see how many processes to create (e.g., from
<code>#SBATCH --ntasks=8</code>).</li>
<li>Launch that many copies of your executable.</li>
<li>Manage where these processes run. If you request resources across
multiple nodes, the launcher (in coordination with Slurm) starts the
processes on the correct machines and ensures they can all communicate
with each other.</li>
</ol><p>The following is an example of how you would compile and launch an
MPI program on Iridis X.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a><span class="co">#SBATCH --partition=amd</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a><span class="co">#SBATCH --nodes=1</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a><span class="co">#SBATCH --ntasks=8</span></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a><span class="co">#SBATCH --cpus-per-task=1</span></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a><span class="co">#SBATCH --time=00:01:00</span></span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a><span class="co"># Load the specific MPI library we want to use</span></span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a><span class="co"># This makes the 'mpicc' and 'mpirun' commands available</span></span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a><span class="ex">module</span> load openmpi</span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a><span class="co"># Compile the code using the MPI compiler wrapper</span></span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a><span class="ex">mpicc</span> vector_mpi.c <span class="at">-o</span> vector_mpi.exe</span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a><span class="co"># Launch the program using 'srun'. This is the recommended</span></span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a><span class="co"># launcher on Slurm clusters. 'srun' automatically reads the</span></span>
<span id="cb8-18"><a href="#cb8-18" tabindex="-1"></a><span class="co"># SBATCH settings and launches 8 (from --ntasks) copies</span></span>
<span id="cb8-19"><a href="#cb8-19" tabindex="-1"></a><span class="co"># of our executable</span></span>
<span id="cb8-20"><a href="#cb8-20" tabindex="-1"></a><span class="ex">srun</span> ./vector_mpi.exe</span>
<span id="cb8-21"><a href="#cb8-21" tabindex="-1"></a></span>
<span id="cb8-22"><a href="#cb8-22" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span>
<span id="cb8-23"><a href="#cb8-23" tabindex="-1"></a><span class="co"># An alternative way to launch the program</span></span>
<span id="cb8-24"><a href="#cb8-24" tabindex="-1"></a><span class="co"># 'mpirun' is the more traditional MPI launcher. Here, we</span></span>
<span id="cb8-25"><a href="#cb8-25" tabindex="-1"></a><span class="co"># must explicitly tell it how many processes (-np) to run</span></span>
<span id="cb8-26"><a href="#cb8-26" tabindex="-1"></a><span class="co"># We use the $SLURM_NTASKS variable (which Slurm sets to 8)</span></span>
<span id="cb8-27"><a href="#cb8-27" tabindex="-1"></a><span class="co"># to match our resource request</span></span>
<span id="cb8-28"><a href="#cb8-28" tabindex="-1"></a><span class="co"># mpirun -np $SLURM_NTASKS ./vector_mpi.exe</span></span>
<span id="cb8-29"><a href="#cb8-29" tabindex="-1"></a><span class="co"># -----------------------------------------------------------</span></span></code></pre>
</div>
<div id="hybrid-mpiopenmp" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="hybrid-mpiopenmp" class="callout-inner">
<h3 class="callout-title">Hybrid MPI+OpenMP</h3>
<div class="callout-content">
<p>MPI and OpenMP don’t have to be competing choices. They can be used
together in a hybrid parallel model, where MPI distributes work across
nodes and OpenMP manages threads within each node. This combination
allows applications to scale beyond a single machine while using memory
and CPU cores more efficiently.</p>
<p>Hybrid parallelism reduces data duplication between processes and
improves load balancing through OpenMP’s flexible scheduling. It also
helps lower communication costs by keeping shared-memory operations
local to a node.</p>
<p>The main drawbacks are added complexity and potential overheads from
managing both models. Code becomes harder to write, debug, and port
between systems. Still, hybrid MPI+OpenMP programs are often the best
solution for large-scale workloads where pure MPI or OpenMP alone falls
short.</p>
</div>
</div>
</div>
<div id="challenge2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Why does using a hybrid MPI+OpenMP scheme reduce data
duplication?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>In a pure MPI application, you would run many separate processes on a
single compute node (e.g., 64 processes on 64 cores). Each MPI process
has its own private memory. If all 64 processes need to access the same
10GB input file, that 10GB of data must be loaded 64 times, once into
each process’s memory. This duplicates the data 64 times, using 640GB of
RAM on that node just for that one file.</p>
<p>In a hybrid MPI+OpenMP model, you may run only one MPI process on
that node. That single process would then use OpenMP to spawn 64
threads, one for each core. Because all OpenMP threads share the memory
of their parent process, the 10GB input file only needs to be loaded
once. All 64 threads can access that single copy, reducing the memory
footprint for that data from 640GB to just 10GB.</p>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="using-gpus-instead-of-cpus">Using GPUs instead of CPUs<a class="anchor" aria-label="anchor" href="#using-gpus-instead-of-cpus"></a></h2>
<hr class="half-width"><p>Besides using multiple CPUs, we can also use Graphical Processing
Units (GPUs) to do calculations in parallel. GPUs were originally
designed to speed up rendering to display images to a screen, a task
that involves performing millions of simple, repetitive calculations in
parallel. Researchers soon realised this design was also perfect for
many scientific problems.</p>
<p>GPUs are highly parallel, built to perform thousands of operations at
the same time. This makes them ideal for work that can be split into
many identical, independent tasks. While CPUs are designed to tackle
complex tasks one after another, GPUs are optimised for doing the exact
same operation on large amounts of data simultaneously. This is perfect
for problems like matrix operations, where every element can be
processed in the same way.</p>
<p>However, <em>offloading</em> work to a GPU is more complicated than
parallelising using CPUs. The main reason is that the CPU and the GPU
have their own separate memory spaces. Data stored in the CPU’s memory
is not visible to the GPU, and vice-versa. This setup is similar to the
separate memory for each process in an MPI application. Data must be
copied from the CPU’s memory to the GPU’s memory. This transfer step is
slow compared to the speed of the calculations. Therefore, efficient GPU
programs must minimise data transfers, often by keeping data on the GPU
as long as possible. While optimising CPU code often focuses on reducing
the total number of calculations, optimising GPU code is usually more
about reducing data transfers and organising data efficiently in the
GPU’s memory.</p>
<p>This level of detail is beyond the scope of this introduction. As
before, we will only give a broad overview of two popular frameworks:
OpenACC and CUDA. You can think of these as being similar to OpenMP and
MPI:</p>
<ul><li>OpenACC is like OpenMP: you can often add it to existing code, using
compiler directives to automatically handle the parallelisation.</li>
<li>CUDA is “similar” to MPI: it is a more complex framework that
requires you to design your program around it.</li>
</ul><div id="cpu-and-gpu-whats-the-difference" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="cpu-and-gpu-whats-the-difference" class="callout-inner">
<h3 class="callout-title">CPU and GPU, what’s the difference?</h3>
<div class="callout-content">
<figure><img src="fig/cpu_gpu_arch.png" class="figure mx-auto d-block"></figure><p>This diagram illustrates the key difference between a CPU and a GPU.
On the left, the CPU is shown with a few, large “Cores” (green). Each
core is complex, paired with significant “Control” logic (yellow) and
large, fast memory caches (purple and blue). This design makes each CPU
core very “smart” and powerful, ideal for handling complex instructions
and varied tasks one after another.</p>
<p>On the right, the GPU has a completely different structure. It is
composed of hundreds or even thousands of tiny, simple cores (the large
green grid). Notice how much less space is dedicated to “Control” logic
and complex caches. This architecture is not designed for complex,
sequential tasks. Instead, it is a massive parallel workforce, built to
execute the same simple operation (like <code>c[i] = a[i] + b[i]</code>)
at the same time across thousands of different pieces of data. This
“many-core” design is what allows it to perform thousands of operations
concurrently.</p>
<p>GPUs also use a different memory model. Each GPU core can access the
large global memory, which is shared across the entire GPU. It is slower
to read and write to this. To improve performance, groups of cores are
organised into blocks which shared a small, but very fast, memory area.
This setup is similar to shared-memory parallelism on CPUs, where
threads cooperate through a common memory space. However, each GPU
block’s shared memory is private to that block, much like how separate
processes in a distributed-memory model (such as MPI) each have their
own memory and must explicitly exchange data. Efficient GPU programs
manage this hierarchy carefully, reusing shared memory to reduce costly
access to global memory.</p>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="openacc">OpenACC<a class="anchor" aria-label="anchor" href="#openacc"></a></h2>
<hr class="half-width"><p>OpenACC is a framework for parallel programming on GPUs, both for
NVIDIA and AMD GPUs; i.e. it is platform agnostic. Just like OpenMP, it
uses compiler directives to tell the compiler which parts of code should
be executed in parallel on a GPU. Also like OpenMP, the OpenACC runtime
and compiler handles all of the parallelisation details such as
generating the parallel code, transferring data between the CPU and GPU
and synchronising/managing GPU threads.</p>
<p>Whilst most of the heavy lifting for OpenACC is done with compiler
directives, a runtime library is also available for finer control over
things such as selecting which GPU to use (if you are lucky enough to
have more than one!), finer grained data movement and synchronisation
mechanisms. To parallelise our <code>vector_add</code> function to a
GPU, we only need to add a single line of code.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="dt">void</span> vector_add<span class="op">(</span><span class="dt">int</span> <span class="op">*</span>a<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span>b<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span>c<span class="op">,</span> <span class="dt">int</span> n<span class="op">)</span> <span class="op">{</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="co">// This OpenACC directive tells the compiler to parallelise the</span></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a><span class="co">// following 'for' loop, running its iterations concurrently</span></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a><span class="co">// on the GPU</span></span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a><span class="pp">#pragma acc parallel loop</span></span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> n<span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>        c<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> a<span class="op">[</span>i<span class="op">]</span> <span class="op">+</span> b<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<p>The full program is in <a href="files/vector/vector_openacc.c">vector_openacc.c</a>. The directive
<code>#pragma acc parallel loop</code> tells the compiler to parallelise
the loop and execute it on the GPU. Each GPU core/thread performs part
of the vector addition. OpenACC also takes care of allocating memory on
the GPU and transferring data from the CPU to GPU’s memory. It also
copies the results back to the CPU when the loop is completed.</p>
<div id="some-other-useful-directives" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="some-other-useful-directives" class="callout-inner">
<h3 class="callout-title">Some other useful directives</h3>
<div class="callout-content">
<p>As we’ve seen,<code>#pragma acc parallel loop</code> targets a single
loop. OpenACC also provides the <code>#pragma acc kernels</code>
directive. This directive is meant to be put before a larger region of
code, such as a complicated loop or multiple loops. The compiler then
analyses this entire region and automatically determines the best way to
convert the code, including any loops it finds, into parallel “kernels”
of code to run on the GPU.</p>
<p>The main difference is that <code>#pragma acc parallel loop</code> is
prescriptive: you are explicitly telling the compiler to parallelise
that one loop. In contrast, <code>#pragma acc kernels</code> is
descriptive: you are telling the compiler “here is a block of code to
accelerate,” giving it freedom to analyse the code and choose the most
efficient way to parallelise and run it.</p>
<p>Additionally, whilst OpenACC automatically manages data transfers
between the GPU and CPU, more explicit data management is possible using
directives such as <code>#pragma acc data</code>, <code>copy</code>,
<code>copyin</code>, and <code>copyout</code> for more control.</p>
</div>
</div>
</div>
<p>To compile an OpenACC program, we need a compiler that supports it.
On Iridis X this is either GCC or one from NVIDIA’s compiler collection.
With GCC we need to use the <code>-fopenacc</code> flag. However, when
using NVIDIA’s C compiler, <code>nvc</code>, the flag is instead
<code>-acc</code>, e.g. <code>nvc -acc vector_openacc.c</code>. Without
the flag, the OpenACC directives are ignored and GPU code is not
generated.</p>
<p>The following is an example of how you would compile and launch an
OpenACC program on Iridis X. It is especially important that we remember
to request a GPU using the <code>--gres=gpu:1</code> directive and
select a partition where nodes have GPUs.</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a><span class="co">#SBATCH --partition=l4</span></span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a><span class="co">#SBATCH --nodes=1</span></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a><span class="co">#SBATCH --ntasks=1</span></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a><span class="co">#SBATCH --cpus-per-task=1</span></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a><span class="co">#SBATCH --gres=gpu:1</span></span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a><span class="co">#SBATCH --time=00:01:00</span></span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a><span class="co"># Load the NVIDIA HPC SDK module, which provides OpenACC support</span></span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a><span class="ex">module</span> load nvhpc</span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a><span class="co"># Compile the program with GPU offloading enabled</span></span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a><span class="ex">nvc</span> <span class="at">-acc</span> vector_openacc.c <span class="at">-o</span> vector_openacc.exe</span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a><span class="co"># Check GPU availability and run the program</span></span>
<span id="cb10-17"><a href="#cb10-17" tabindex="-1"></a><span class="ex">nvidia-smi</span></span>
<span id="cb10-18"><a href="#cb10-18" tabindex="-1"></a><span class="ex">./vector_openacc.exe</span></span></code></pre>
</div>
<p>The main advantage of OpenACC, like OpenMP, is that it allows rapid
GPU parallelisation with minimal code changes. It is particularly useful
for incrementally porting existing CPU applications to GPUs. However,
compared to lower-level frameworks like CUDA, it offers less
fine-grained control over GPU execution and memory management. OpenACC
is therefore well suited for scientific and engineering applications
where productivity are more important than maximal performance.</p>
</section><section><h2 class="section-heading" id="cuda">CUDA<a class="anchor" aria-label="anchor" href="#cuda"></a></h2>
<hr class="half-width"><p>The final framework we will look at is CUDA, which is essentially a
programming language created by NVIDIA to run arbitrary code on NVIDIA
GPUs. However, unlike OpenACC which uses compiler directives to
automatically parallalise sections of code, CUDA is an extension of the
C/C++ language which gives you explicit <em>fine-grained</em> control
over the GPU. This is a much more powerful and performant approach, but
also far more complex. You are no longer advising the compiler; you are
directly writing the code that will run on the GPU. Given this
complexity, we will only examine the key concepts at a very high
level.</p>
<p>In CUDA, you write special functions called <em>kernels</em> that are
executed on the GPU. In the later code example below, we have two
functions: <code>vector_add_kernel</code> and <code>vector_add</code>.
The <code>vector_add_kernel</code> function is the code which runs on
the GPU. The <code>__global__</code> keyword tells the compiler that
this function should be launched from the CPU but run on the GPU. Inside
this kernel, each thread calculates its own unique ID using special
variables (like <code>blockIdx.x</code> and <code>threadIdx.x</code>).
This ID determines which element <code>i</code> of the vector
<code>c</code> that specific thread will process. This is the core
mechanism for dividing the parallel work. You will also see an
<code>if (i &lt; n)</code> check in the kernel. This is a crucial safety
check. GPUs launch threads in fixed-size groups, so you often launch
more threads than you have data (e.g., launching 1024 threads for a
1000-element array). This <code>if</code> statement simply tells the
“extra” threads (1000, 1001, etc.) to do nothing. Without it, they would
try to access memory that doesn’t exist, which could corrupt your data
or crash the program.</p>
<p>The original <code>vector_add</code> functions runs on the CPU and
manages the GPU, controlling when the GPU kernel is launched. It must
now do all the work <em>manually</em> that OpenACC handled
automatically:</p>
<ol style="list-style-type: decimal"><li>Allocate Memory: <code>cudaMalloc</code> is used to allocate
separate memory for the three vectors (<code>d_a</code>,
<code>d_b</code>, <code>d_c</code>) <em>on the GPU’s memory</em>. The
<code>d_</code> prefix is a common convention to mean “device”
memory.</li>
<li>Copy Data In: <code>cudaMemcpy</code> (with
<code>cudaMemcpyHostToDevice</code>) is called to copy the input data
from the CPU’s <code>a</code> and <code>b</code> arrays to the GPU’s
<code>d_a</code> and <code>d_b</code> arrays. This is the explicit data
transfer.</li>
<li>Launch Kernel: The
<code>&lt;&lt;&lt;grid, BLOCK_SIZE&gt;&gt;&gt;</code> syntax is the
CUDA-specific command to launch the kernel. This tells the GPU to launch
a “grid” of “blocks,” to execute the <code>vector_add_kernel</code>
function.</li>
<li>Copy Data Out: After the kernel finishes, <code>cudaMemcpy</code>
(with <code>cudaMemcpyDeviceToHost</code>) copies the result from the
GPU’s <code>d_c</code> array back to the CPU’s <code>c</code>
array.</li>
<li>Clean Up: <code>cudaFree</code> releases the memory on the GPU.</li>
</ol><div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">CPP<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode cpp" tabindex="0"><code class="sourceCode cpp"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="co">// This is the "kernel" - the code that runs on the GPU a.k.a. the device</span></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>__global__ <span class="dt">void</span> vector_add_kernel<span class="op">(</span><span class="dt">int</span> <span class="op">*</span>a<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span>b<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span>c<span class="op">,</span> <span class="dt">int</span> n<span class="op">)</span> <span class="op">{</span></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>    <span class="co">// Calculate the unique ID for this thread</span></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>    <span class="dt">int</span> i <span class="op">=</span> blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>    <span class="co">// Ensure the thread ID is within the bounds of the array</span></span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>i <span class="op">&lt;</span> n<span class="op">)</span> <span class="op">{</span></span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a>        c<span class="op">[</span>i<span class="op">]</span> <span class="op">=</span> a<span class="op">[</span>i<span class="op">]</span> <span class="op">+</span> b<span class="op">[</span>i<span class="op">];</span></span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a><span class="co">// This is the "host" function - the code that runs on the CPU</span></span>
<span id="cb11-13"><a href="#cb11-13" tabindex="-1"></a><span class="dt">void</span> vector_add<span class="op">(</span><span class="dt">int</span> <span class="op">*</span>a<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span>b<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span>c<span class="op">,</span> <span class="dt">int</span> n<span class="op">)</span> <span class="op">{</span></span>
<span id="cb11-14"><a href="#cb11-14" tabindex="-1"></a>    <span class="co">// Pointers for the "device" (GPU) memory</span></span>
<span id="cb11-15"><a href="#cb11-15" tabindex="-1"></a>    <span class="dt">int</span> <span class="op">*</span>d_a<span class="op">,</span> <span class="op">*</span>d_b<span class="op">,</span> <span class="op">*</span>d_c<span class="op">;</span></span>
<span id="cb11-16"><a href="#cb11-16" tabindex="-1"></a>    <span class="dt">int</span> size <span class="op">=</span> n <span class="op">*</span> <span class="kw">sizeof</span><span class="op">(</span><span class="dt">int</span><span class="op">);</span></span>
<span id="cb11-17"><a href="#cb11-17" tabindex="-1"></a></span>
<span id="cb11-18"><a href="#cb11-18" tabindex="-1"></a>    <span class="co">// 1. Allocate memory on the GPU</span></span>
<span id="cb11-19"><a href="#cb11-19" tabindex="-1"></a>    cudaMalloc<span class="op">(&amp;</span>d_a<span class="op">,</span> size<span class="op">);</span></span>
<span id="cb11-20"><a href="#cb11-20" tabindex="-1"></a>    cudaMalloc<span class="op">(&amp;</span>d_b<span class="op">,</span> size<span class="op">);</span></span>
<span id="cb11-21"><a href="#cb11-21" tabindex="-1"></a>    cudaMalloc<span class="op">(&amp;</span>d_c<span class="op">,</span> size<span class="op">);</span></span>
<span id="cb11-22"><a href="#cb11-22" tabindex="-1"></a></span>
<span id="cb11-23"><a href="#cb11-23" tabindex="-1"></a>    <span class="co">// 2. Copy input data from CPU (host) to GPU (device)</span></span>
<span id="cb11-24"><a href="#cb11-24" tabindex="-1"></a>    cudaMemcpy<span class="op">(</span>d_a<span class="op">,</span> a<span class="op">,</span> size<span class="op">,</span> cudaMemcpyHostToDevice<span class="op">);</span></span>
<span id="cb11-25"><a href="#cb11-25" tabindex="-1"></a>    cudaMemcpy<span class="op">(</span>d_b<span class="op">,</span> b<span class="op">,</span> size<span class="op">,</span> cudaMemcpyHostToDevice<span class="op">);</span></span>
<span id="cb11-26"><a href="#cb11-26" tabindex="-1"></a></span>
<span id="cb11-27"><a href="#cb11-27" tabindex="-1"></a>    <span class="co">// 3. Launch the kernel on the GPU</span></span>
<span id="cb11-28"><a href="#cb11-28" tabindex="-1"></a>    <span class="dt">int</span> BLOCK_SIZE <span class="op">=</span> <span class="dv">256</span><span class="op">;</span></span>
<span id="cb11-29"><a href="#cb11-29" tabindex="-1"></a>    <span class="dt">int</span> grid <span class="op">=</span> <span class="op">(</span>n <span class="op">+</span> BLOCK_SIZE <span class="op">-</span> <span class="dv">1</span><span class="op">)</span> <span class="op">/</span> BLOCK_SIZE<span class="op">;</span></span>
<span id="cb11-30"><a href="#cb11-30" tabindex="-1"></a>    vector_add_kernel<span class="op">&lt;&lt;&lt;</span>grid<span class="op">,</span> BLOCK_SIZE<span class="op">&gt;&gt;&gt;(</span>d_a<span class="op">,</span> d_b<span class="op">,</span> d_c<span class="op">,</span> n<span class="op">);</span></span>
<span id="cb11-31"><a href="#cb11-31" tabindex="-1"></a></span>
<span id="cb11-32"><a href="#cb11-32" tabindex="-1"></a>    <span class="co">// 4. Copy the result from GPU (device) back to CPU (host)</span></span>
<span id="cb11-33"><a href="#cb11-33" tabindex="-1"></a>    cudaMemcpy<span class="op">(</span>c<span class="op">,</span> d_c<span class="op">,</span> size<span class="op">,</span> cudaMemcpyDeviceToHost<span class="op">);</span></span>
<span id="cb11-34"><a href="#cb11-34" tabindex="-1"></a></span>
<span id="cb11-35"><a href="#cb11-35" tabindex="-1"></a>    <span class="co">// 5. Free the memory on the GPU</span></span>
<span id="cb11-36"><a href="#cb11-36" tabindex="-1"></a>    cudaFree<span class="op">(</span>d_a<span class="op">);</span></span>
<span id="cb11-37"><a href="#cb11-37" tabindex="-1"></a>    cudaFree<span class="op">(</span>d_b<span class="op">);</span></span>
<span id="cb11-38"><a href="#cb11-38" tabindex="-1"></a>    cudaFree<span class="op">(</span>d_c<span class="op">);</span></span>
<span id="cb11-39"><a href="#cb11-39" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<p>The full program is in <a href="files/vector/vector_cuda.cu">vector_cuda.cu</a>. To compile CUDA
code, we must use NVIDIA’s <code>nvcc</code> compiler. Note that this is
different from the <code>nvc</code> compiler we used to build the
OpenACC version of <code>vector_add</code>. The <code>nvcc</code>
compiler understands the CUDA-specific syntax used to define and launch
kernels, e.g. <code>__global__</code> and
<code>&lt;&lt;&lt;...&gt;&gt;&gt;</code>. It separates out the
GPU-specific and CPU-specific code, compiling the CPU code using a
standard C++ compiler. CUDA files typically use the .cu extension.</p>
<p>To run a CUDA program, it is launched like any other program, as the
mechanics to communicate and launch threads on the GPU is handled inside
the program itself. The submission script for a CUDA code is nearly
identical to the OpenACC script from before.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="co"># !/bin.bash</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="co"># SBATCH --partition=a100</span></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="co"># SBATCH --nodes=1</span></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a><span class="co"># SBATCH --ntasks=1</span></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a><span class="co"># SBATCH --gres=gpu:1</span></span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a><span class="co"># SBATCH --time=00:01:00</span></span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a><span class="co"># Load the NVIDIA CUDA toolkit module</span></span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a><span class="ex">module</span> load cuda</span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a><span class="co"># Compile the program using nvcc</span></span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a><span class="ex">nvcc</span> vector_cuda.cu <span class="at">-o</span> vector_cuda.exe</span>
<span id="cb12-13"><a href="#cb12-13" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" tabindex="-1"></a><span class="co"># Run the executable (no special launcher needed)</span></span>
<span id="cb12-15"><a href="#cb12-15" tabindex="-1"></a><span class="ex">./vector_cuda.exe</span></span></code></pre>
</div>
<p>The main benefit of CUDA is the total control it provides, which
often leads to the highest possible performance. The main drawbacks are
its complexity (requiring manual memory management and kernel writing)
and vendor lock-in, as CUDA code will only run on NVIDIA GPUs.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul><li>Any language can be used for HPC, however, compiled languages like
C, C++ and Fortran are typically used for performance-critical code.
Python is often used on HPC using libraries which are built on compiled
languages.</li>
<li>OpenMP is a shared-memory model, using compiler directives
(<code>#pragma omp</code>) to easily parallelise code (often loops) to
run on the CPU cores of a single node.</li>
<li>MPI is a distributed-memory model, using a library of functions
(<code>MPI_Init</code>, <code>MPI_Send</code>, etc.) to manage explicit
communication between processes. It is complex but can scale across many
nodes.</li>
<li>GPUs are “many-core” processors ideal for massive, simple, parallel
tasks (like matrix maths). Using them requires copying data between the
CPU (host) and GPU (device).</li>
<li>OpenACC uses compiler directives (<code>#pragma acc</code>) to
offload work to a GPU, automating parallelisation and data
transfers.</li>
<li>CUDA is a complex, explicit programming model for NVIDIA GPUs. It
requires you to write “kernels” and manually manage memory
(<code>cudaMalloc</code>, <code>cudaMemcpy</code>) but offers the
highest control and performance.</li>
</ul></div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</section></div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="4_programmatic_parallelism.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="6_parallel_scaling.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="4_programmatic_parallelism.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Introduction to
        </a>
        <a class="chapter-link float-end" href="6_parallel_scaling.html" rel="next">
          Next: Measuring and...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/carpentries/workbench-template-md/edit/main/episodes/5_hpc_technologies.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/carpentries/workbench-template-md/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/carpentries/workbench-template-md/" class="external-link">Source</a></p>
				<p><a href="https://github.com/carpentries/workbench-template-md/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.17.1" class="external-link">sandpaper (0.17.1)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.7" class="external-link">varnish (1.0.7)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "LearningResource",
  "@id": "https://carpentries.github.io/workbench-template-md/5_hpc_technologies.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/LearningResource/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "Landscape of HPC Technologies",
  "creativeWorkStatus": "active",
  "url": "https://carpentries.github.io/workbench-template-md/5_hpc_technologies.html",
  "identifier": "https://carpentries.github.io/workbench-template-md/5_hpc_technologies.html",
  "dateCreated": "2025-11-10",
  "dateModified": "2025-11-06",
  "datePublished": "2025-11-10"
}

  </script><script>
		feather.replace();
	</script></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

